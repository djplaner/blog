---
title: Choosing your indicators - why, how and what
date: 2008-08-31 12:52:38+10:00
categories: ['blackboardindicators', 'c2d2', 'complexityleadership', 'coursesites', 'elearning']
type: post
template: blog-post.html
comments:
    - approved: '1'
      author: Tony Bowes
      author_email: vrbones@gmail.com
      author_ip: 150.101.181.34
      author_url: null
      content: "More toys I can't play with :/\n\nDuring one of the GDLT sessions I had\
        \ caught wind of the fact that the system tracks what pages are being viewed and\
        \ by whom. When the course has been completed it would be great getting some \"\
        google analytics\" type metrics out showing the page's view histry together with\
        \ time on page, entry / exit points, etc. With student tracking it would be interesting\
        \ seeing if any of the metrics correlate to final grade.\n\nMore interesting metrics\
        \ would be:\n - supervisor post count on forum\n - post count per day overlaid\
        \ with critical dates (assessment due, etc)\n - student / supervisor post count\
        \ ratio on forum.\n - posts / readers ratio\n\nNot sure if it's a metric, but\
        \ one thing I have been noticing more recently is a purposeful attempt in the\
        \ coursework to get the student to post feedback or discuss an item on the forum.\
        \ These points not only generate traffic and lively discussion, but also give\
        \ an indication of where everyone is up to in the course. Having at least 2 or\
        \ 3 of these points in each session would keep the engagement going. Ideally I'd\
        \ like to see either a hyperlinking system so that you can leave forum comments\
        \ tagged back to specific content, or even a comment-in-place system. Guess that's\
        \ more for another topic tho..."
      date: '2008-09-01 17:13:53'
      date_gmt: '2008-09-01 07:13:53'
      id: '1597'
      parent: '0'
      type: comment
      user_id: '0'
    - approved: '1'
      author: d.jones
      author_email: d.jones@cqu.edu.au
      author_ip: 59.154.24.147
      author_url: http://cq-pan.cqu.edu.au/david-jones/
      content: 'G''day Tony,
    
    
        Thanks for the comment.   A lot to consider and we''ve though of some of these.
    
    
        The integration of online activities into the course is an obvious one to encourage,
        rather than leave it as an optional thing.   Some of the work we''ve done around
        constructive alignment speaks to that.
    
    
        The Google Analytics type information is certainly something that would help the
        staff and there are some that try and use it. But it probably also needs to be
        integrated into what they do.  Sort of applying constructive alignment into the
        processes that support e-learning.
    
    
        David.'
      date: '2008-09-02 07:45:21'
      date_gmt: '2008-09-01 21:45:21'
      id: '1598'
      parent: '0'
      type: comment
      user_id: '0'
    
pingbacks:
    []
    
---
The [unit](http://cddu.cqu.edu.au/) I work with is undertaking a project called [Blackboard Indicators](http://cddu.cqu.edu.au/index.php/Blackboard_Indicators). Essentially the development of a tool that will perform some automated checks on our [institution's](http://www.cquni.edu.au/) Blackboard course sites and show some indicators which might identify potential problems or areas for improvement.

The current status is that we're starting to develop a slightly better idea of what people are currently doing through use of the literature and also some professional networks (e.g. the [Australasian Council on Open, Distance and E-learning](http://www.acode.edu.au/)) and have an initial prototype running.

Our current problem is how do you choose what the indicators should be? What are the types of problems you might see? What is a "good" course website?

### Where are we up to?

Our initial [development work](http://cddu.cqu.edu.au/index.php/Category:Blackboard_Indicators) has focused on three groupings of category: course content, coordinator presence and all interactions. Some more detail on this [previous post](http://cq-pan.cqu.edu.au/david-jones/blog/?p=204).

[Colin Beer](http://cddu.cqu.edu.au/index.php/Colin_Beer) has contributed some additional thinking about some potential indicators in a [recent post](http://beerc.wordpress.com/2008/08/30/lms-inidcators-project/) on his blog.

Col and I have talked about using our blogs and other locations to talk through what we're thinking to develop a concrete record of our thoughts and hopefully generate some interest from other folk.

Col's list includes

- Learner.
- Instructor.
- Content.
- Interactions: learner/learner, learner/instructor, learner/content, instructor/content

### Why and what?

In identifying a list of indicators, as when trying to evaluate anything, it's probably a good idea to start with a clear definition of why you are starting on this, what are you trying to achieve.

The stated purpose of this project is to help us develop a better understanding of how and how well staff are using the Blackboard courses sites. In particular, we want to know about any potential problems (e.g. a course site not being available to students) that might cause a large amount of "helpdesk activity". We would also like to know about trends across the board which might indicate the need for some staff development, improvements in the tools or some support resources to improve the experience of both staff and students.

There are many other aims which might apply, but this is the one I feel most comfortable with, at the moment.

Some of the other aims include

- Providing academic staff with a tool that can aid them during course site creation by checking their work and offering guidance on what might be missing.
- Provide management with a tool to "check on" course sites they are responsible for.
- Identify correlations between characteristics of a course website and success.

The constraints we need to work within include

- Little or no resources - implication being that manual, human checking of course sites is not currently a possibility.
- Difficult organisational context due to on-going restructure - which makes it hard to get engagement from staff in a task that is seen as additional to existing practice and also suggests a need to be helping staff deal with existing problems more so than creating more work. A need to be seen to be working with staff to improve and change, rather than being seen as inflicting change upon them.
- LMS will be changing - come 2010 we'll be using a new LMS, whatever we're doing has to be transportable.

### How?

From one perspective there are two types of process which can be used in a project like this

1. Teleological or idealist.  
    A group of experts get together, decide and design what is going to happen and then explain to everyone else why they should use it and seek to maintain obedience to that original design.
2. Ateleological or naturalist.  
    A group of folk, including significant numbers of folk doing real work, collaborate together to look at the current state of the local context and undertake a lot of small scale experiments to figure out if anything makes sense, they examine and reflect on those small scale experiments and chuck out the ones that didn't work and build on the ones that did.

(For more on this check out: this [presentation video](http://video.google.com/videoplay?docid=-5567968733907010214&hl=en) or this [presentation video](http://video.google.com/videoplay?docid=3241775154462848671&hl=en) or this [paper](http://www.ascilite.org.au/conferences/singapore07/procs/jones-d.pdf) or this [one](http://cq-pan.cqu.edu.au/david-jones/Publications/Papers_and_Books/).)

From the biased way I explained the choices I think it's fairly obvious which approach I prefer. A preference for the atelelogical approach also means that I'm not likely to want to spend vast amounts of time evaluating and designing criteria based on my perspectives. It's more important to get a set of useful indicators up and going, in a form that can be accessed by folk and have a range of processes by which discussion and debate is encouraged and then fed back into the improvement of the design.

The on-going discussion about the project is more likely to generate something more useful and contextually important than large up-front analysis.

### What next then?

As a first step, we have to get something useful (for both us and others) up and going in a form that is usable and meaningful. We then have to engage with them and find out what they think and where they'd like to take it next. In parallel with this is the idea of finding out, in more detail, what other institutions are doing and see what we can learn.

The engagement is likely going to need to be aimed at a number of different communities including

- Quality assurance folk: most Australian universities have quality assurance folk charged with helping the university be seen by [AUQA](http://www.auqa.edu.au/) as being good.  
    This will almost certainly, eventually, require identifying what are effective/good outcomes for a course website as outcomes are a main aim for the next AUQA round.
- Management folk: the managers/supervisors at CQU who are responsible for the quality of learning and teaching at CQU.
- Teaching staff: the people responsible for creating these artifacts.
- Students: for their insights.

Initially, the indicators we develop should match our stated aim - to identify problems with course sites and become more aware with how they are being used. To a large extent this means not worrying about potential indicators of good outcomes and whether or not there is a causal link.

I think we'll start discussing/describing the indicators we're using and thinking about on a [project page](http://cddu.cqu.edu.au/index.php/Types_of_Blackboard_Indicators) and we'll see where we go from there.