---
categories:
- edc3100
- teaching
date: 2012-12-14 17:54:11+10:00
next:
  text: Why Moneyball is the wrong analogy for learning analytics
  url: /blog2/2012/12/17/why-moneyball-is-the-wrong-analogy-for-learning-analytics/
previous:
  text: Developing personal learning networks for open and social learning
  url: /blog2/2012/12/14/4668/
title: '"Beyond the early adopters of online instruction: Motivating the reluctant
  majority"'
type: post
template: blog-post.html
comments:
    []
    
pingbacks:
    - approved: '1'
      author: 'Beyond the early adopters of online instruction: Motivating the ... | Quality
        Assurance in Online Courses | Scoop.it'
      author_email: null
      author_ip: 89.30.105.121
      author_url: http://www.scoop.it/t/quality-assurance-in-online-courses/p/3711519111/beyond-the-early-adopters-of-online-instruction-motivating-the
      content: '[...] There are a number of universities in Australia where the majority
        of, if not all, courses are taught online. These institutions already have the
        &ldquo;next wave&rdquo; online. The problem though is that the quality of the
        online experience ...&nbsp; [...]'
      date: '2012-12-14 23:55:34'
      date_gmt: '2012-12-14 13:55:34'
      id: '537'
      parent: '0'
      type: pingback
      user_id: '0'
    
---
The following is a summary and some reflection on Hixon et al (2012). I'm particularly interested in this topic due to my belief (based on 20 years experience and observation) that most institutional approaches to change in learning and teaching has only been successful in moving the same 10% of staff. A 10% that didn't need a lot of help in the first place.

## Thoughts and to do

Fairly disappointed with this paper. Didn't engage at all with the perspective of Geoghegan (1994) who took the implications from the adopters categories a lot further and questioned some of the fundamental assumptions.

Misc thoughts, questions and to do

- Is there anyone doing interesting research/thinking around the inherent diversity in academics and the inherent consistency in what passes for institutional e-learning?  
    Looking at the work referencing Geoghegan would probably be a good start.
- How/what does the increasingly universal adoption of e-learning in Oz Universities imply for the adopter categories and from there how e-learning is supported and the subsequent quality of it?
- Can learning analytics of LMS usage identify/support the adopter categories? Or at the very least some difference between staff?
- Look at the conceptual paper (Barczyk et al, 2010) that informed the survey

In the following, where I remember, my comments are _emphasised_. Other text is a summary of Hixon et al (2012)

## Abstract

> Now that most of the innovators and early adopters of online instruction are comfortably teaching online, many institutions are facing challenges as they prepare the next wave of online instructors. This research how faculty in this “next wave” (the majority of adopters) differ from the innovators and early adopters of online instruction. A specific online course development program is described and the experiences of the “majority” in the program are examined in relation to the experiences of previous participants (the innovators and early adopters).

_There is probably a refinement to be made here. There are a number of universities in Australia where the majority of, if not all, courses are taught online. These institutions already have the "next wave" online. The problem though is that the quality of the online experience leaves a great deal to be desired._

## Introduction

More folk have to move online. This study designed to help inform best practices "in bringing the 'majority' online"

Based on the Distance Education Mentoring program at a Midwestern university. Cohort-based mentoring program to help faculty develop an online course. Over four years of operation it's been noted that faculty participants are changing. Looks at Rogers' Diffusion of Innovation theory to understand the changes. A few paragraphs explaining DOI getting to categories of adopters. Moves onto some literature on those using it exploring technological innovations.

_Interestingly, they don't reference Geoghegan (1994) who used Moore's extension of these to make some points along these lines. Interesting largely because Geoghegan is one of my theoretical/literature "hammers"._

### Approaches to development of online courses

Posits two approaches

1. faculty-driven approach;  
    An approach that can work if faculty have the skills.
2. collaborative approach.  
    Seen as the solution to overcome the difficulties (especially pedagogical) of the faculty-driven approach.

_I'd argue that the same observation (lack of skills) can be made with the collaborative approach. I've been in situations where the "collaborators" haven't had the necessary skills either. What isn't explicitly noted in the above is that both normally assume that course development is separate from teaching. A course is redesigned before/after teaching._

Struggles faced by academics from the literature include

- learning the necessary skills;  
    _A contributing factor here is the really poor quality of the technical tools. Some of that difficulty arises because the tools are from another context and don't match the local context._
- adapting the pedagogic strategies for the online environment;  
    _At the same time workload formulas, room allocation, legal requirements, policies and student prejudices mitigate against the adoption of those pedagogical strategies._
- conceptualising their course for the new environment;
- finding increased time required to develop quality online courses.  
    _Wonder if the faculty had developed "quality" face-to-face courses? What's the source of this difficulty online?_

_The assumption here is that it is course development that must be collaborative. What would it look like, how would it happen and what would be the impacts if the course delivery process was collaborative? i.e. don't assume that faculty skill-development and course redesign only occurs before the course is taught. What happens if as I'm teaching the course I make changes and am able to learn more about what works. More importantly, that the organisational e-learning systems/policies/processes can learn more._

## Method

The mentoring program "is design to educate and certify faculty members in the principles of instructional design". Each faculty participant (protege) has a mentor from outside the discipline. Uses the Quality Matters rubric. More detail given on how it works.

Courses are taught and then evaluated and given a pass/fail based on the rubric.

By the 3rd year of the program, noticed participants "are hesitant, or even resistant, to consider new approaches and technologies, or even to teach online". Which is argued to be fitting to the idea that they are "the majority". _Would be interesting to dig further into this? Were this "majority" being required to participate?_

Program changed in fourth year. More structure. More defined structure in the online course they complete. Submission by specific deadlines. Formal meeting schedule. A contract required to be signed. _Mmm, not a great fan of that. I wonder if they actually looked at when Rogers and others have said about the characteristics of the majority and if the changes to the program were based on those insights? e.g. their communication networks tend to be vertically oriented, wouldn't having a mentor from outside the discipline be a poor match?_

### Research questions

1. In what ways do faculty members participating in the 4th offering differ from prior offerings?
2. In what ways do the experiences/perceptions of the 4th years differ?

Survey questionnaire developed to ask: skill development, mentoring relationship, its effectiveness, perceptions of teaching as a result, program satisfaction, general beliefs about online education. _Wouldn't connecting this to DOI have been sensible?_

47 of the 92 proteges completed the survey: 27% of year 1, 52% of year 2, 57% of year 3, 58% of year 4.

Responses from years 1-3 combined to compare. _But they've said they noticed differences in year 3?_

## Results

Those in 1st three years had been teaching longer than 4th years. Simlarly, the earlier group had higher ranks. _Oh dear, it appears the 4th years might "junior" academics fighting to establish themselves as researchers forced to engage with the program_

Year 4 less likely to identify as early adopters of technology. _Continuing stereotypes would have required age to have been mentioned by now wouldn't it?_ No significant difference in age.

Both groups were equally looking forward to the program.

Year 4 group reported more benefit from online course. _Well this measures the changes in the course, rather than the people_ Both groups satisfied similarly with program.

## References

Barczyk, C., Buckenmeyer, J., & Feldman, L. (2010). Mentoring professors: A model for developing quality online instructors and courses in higher education. International Journal on E-Learning, 9(1), 7–26.

Geoghegan, W. (1994). Whatever happened to instructional technology? In S. Bapna, A. Emdad, & J. Zaveri (Eds.), (pp. 438–447). Baltimore, MD: IBM.

Hixon, E., Buckenmeyer, J., Barczyk, C., Feldman, L., & Zamojski, H. (2012). Beyond the early adopters of online instruction: Motivating the reluctant majority. The Internet and Higher Education, 15(2), 102–107. doi:10.1016/j.iheduc.2011.11.005