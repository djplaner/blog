---
title: The emperor has no clothes - why is the learning and teaching peformance fund naked
date: 2009-02-13 09:29:09+10:00
categories: ['chapter-2', 'elearning', 'evaluation']
type: post
template: blog-post.html
comments:
    []
    
pingbacks:
    - approved: '1'
      author: Further evidence of problems with evaluation of teaching &laquo; The Weblog
        of (a) David Jones
      author_email: null
      author_ip: 74.200.243.208
      author_url: https://djon.es/blog/2009/03/02/further-evidence-of-problems-with-evaluation-of-teaching/
      content: '[...] White, N. (2006). &#8220;Tertiary education in the Noughties: the
        student perspective.&#8221; Higher Education Research &amp; Development 25(3):
        231-246. Possibly related posts: (automatically generated)The emperor has no clothes
        - why is the learning and teaching peformance fu&#8230; [...]'
      date: '2009-03-02 21:00:03'
      date_gmt: '2009-03-02 11:00:03'
      id: '2134'
      parent: '0'
      type: pingback
      user_id: '0'
    - approved: '1'
      author: Another perspective for the indicators project &laquo; The Weblog of (a)
        David Jones
      author_email: null
      author_ip: 72.233.96.139
      author_url: https://djon.es/blog/2009/03/12/another-perspective-for-the-indicators-project/
      content: '[...] given that most evaluation of learning at individual Australian
        Universities and within the entire sector rely almost entirely on &#8220;smile
        sheets&#8221; (i.e. low level surveys that test student [...]'
      date: '2009-03-12 13:47:46'
      date_gmt: '2009-03-12 03:47:46'
      id: '2135'
      parent: '0'
      type: pingback
      user_id: '0'
    - approved: '1'
      author: Tell a story about your garden &#8211; narrative and SenseMaker &laquo;
        The Weblog of (a) David Jones
      author_email: null
      author_ip: 66.135.48.209
      author_url: https://djon.es/blog/2009/06/04/tell-a-story-about-your-garden-narrative-and-sensemaker/
      content: '[...] I&#8217;m interested in seeing if this might provide some interesting
        alternatives to the evaluation of learning and teaching. However, apart from seeing
        SenseMaker in action at the workshop I attended and reading about it, I [...]'
      date: '2009-06-04 10:09:02'
      date_gmt: '2009-06-04 00:09:02'
      id: '2136'
      parent: '0'
      type: pingback
      user_id: '0'
    
---

See also: [[blog-home | Home]]

!!! warning "Broken image link"

The Australian Federal Government has a [Learning and Teaching Performance Fund (LTPF)](http://www.dest.gov.au/sectors/higher_education/policy_issues_reviews/key_issues/learning_teaching/ltpf/) that is meant to allocate money to Australian universities on the quality and/or improvement in their learning and teaching.

Based on what I know of this approach I think it is fundamentally broken. It's probably that the ["emperor has no clothes"](http://en.wikipedia.org/wiki/The_Emperor%27s_New_Clothes). i.e. Australian universities know it is broken, but can't point it out because they want to get the money.

### The fund and how it works

According to [this story in the Australian newspaper](http://www.theaustralian.news.com.au/story/0,25197,25043426-12332,00.html) the fund allocated $AUD73 million this year. The [administrative information for providers document](http://www.dest.gov.au/sectors/higher_education/policy_issues_reviews/key_issues/learning_teaching/ltpf/2009ltpf.htm#2009_Administrative_Information_for_Providers) outlines the process for 2009. The process has changed over the 3 years it has been run.

There are two data sources used by the fund:

- [Australian Graduate Survey](http://www.graduateopportunities.com/e_zine/go_for_it_july_07/the_australian_graduate_survey); and  
    All Australian University graduates get a survey in the months after they graduate which asks them two broad sets of questions: are they working and in what, and how satisfied were they with their study/university. The LTPF uses two sets of indicators from this survey
    
    - Student satisfaction indicators
        - Satisfaction with generic skills
        - Satisfaction with good teaching
        - Overall satisfaction
    - Outcome indicators
        - Full-time employment
        - Further full-time and part-time study
    - [Higher education student collection](http://www.dest.gov.au/sectors/higher_education/publications_resources/statistics/chessn.htm).  
        The statistics are used in the LTPF to examine progress rates amongst Bachelor students and the retention rate for the same students.
    
    The process used goes something like
    
    - An adjustment process is applied to the raw indicators data.
    - Each university gets a package describing details of the findings for their students.
    - The university provide a submission offering information that may explain some of the results.
    - An expert panel looks at the information and provides advice to the government, back to the institutions and generally ensures the process is effective.
    
    The trouble is that I think the majority of the data that is at the foundation of this process is less than reliable.
    
    ### Why is it broken
    
    A couple of weeks ago I published [a post](/blog2/2009/01/25/somethings-that-are-broken-with-evaluation-of-university-teaching/) titled "Somethings that are broken with the evaluation of university teaching". Essentially it is a collection of links pointing out that "level 1 smile sheets" (surveys that ask learners "were you satisfied") have significant and well-known limitations in validity and value. In particular the following quote is from [this article](http://www.trainingmag.com/msg/content_display/training/e3iwtqVX4kKzJL%2BEcpyFJFrFA%3D%3D)
    
    > In some instances, there is not only a low correlation between Level I and subsequent levels of evaluation, but a negative one.
    
    From my perspective the course experience questionnaire (the bit of the Australian graduate survey that asks student satisfaction) essentially takes the "level 1 smile sheet" approach and applies it to a graduates entire university experience. I don't see this move to a broader area of coverage (whole university experience, up from individual course/unit/subject) helping address the concerns about "level 1 smile sheets". In fact, I see it getting much worse.
    
    The course experience is likely to cover at least 3 years experience. For some part-time students this might as much as 6, 9 or more years. Do we really believe that their experience over the last 6 to 12 months isn't going to over shadow and be more in their mind than their previous experience?
    
    There are also problems with the graduate destination survey - the part of the survey that asks about what they are doing now. This [article](http://www.articlearchives.com/company-activities-management/management-theory-practice/785252-1.html) points out some of the limitations.
    
    To some extent having institutions comment on the data before the expert panel examines it might address some of this. But I don't think that goes anyway towards addressing the significant limitations of this form of evaluation.
    
    ### Solutions
    
    So if it's broken, what are the alternatives?
    
    I don't know. It's a difficult question and I don't have the knowledge, experience or time to recommend a solution. Given the nature of this problem, I'm not even sure that there is a single correct solution.
    
    Based on what I know, some suggestions I think might be worth more consideration include:
    
    - Measure fit for context/purpose, not comparison.  
        To some extent the LTPF process acknowledges that comparing the quality of learning and teaching across all of the diversity of the Australian higher education sector is extremely difficult. So why continue to try and do it. Why not focus on how well the learning and teaching is for the given context. Measure fit for purpose, not comparison against others. That said, any form of measurement has some potential downsides and negative outcomes.
    - Concentrate on improvement.  
        This is somewhat similar to the last point. It's also linked to one of my common sayings, "It's not how bad you start, but how quickly you get better". Rather than measure fit for context, measure and reward how much better the learning and teaching at a particular institution has become. Also some potential negative concequences.
    - Use other forms of evaluation/data. [This](http://www.llrx.com/columns/guide49.htm) and I'm sure many other places talk about additional forms of evaluation beyond level 1 smile sheets. Dave Snowden also has some interesting approaches to evaluation which might apply.
    
    In general, I would suggest that rather than wringing hands over how difficult, spending inordinate amounts of time reflecting on how hard it is and arguing over which of many options is the best, spending lots of money on consultants that will push their own barrow at the expense of any knowledge of the local context, leaping at the latest fad, or say it's all too hard we can't change the governments mind. I think it would suggest an organisation attempt a lot of [safe-fail probes](http://www.cognitive-edge.com/blogs/dave/2007/11/safefail_probes.php) to investigate different potential solutions. Apply the lessons learned to improve evaluation within the institution and then promote it amongst the sector.